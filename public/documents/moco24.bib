@inproceedings{10.1145/3658852.3658854,
author = {Coleman, Grisha and Batiste, Stephanie L},
title = {Blk as Tek | echo::system and Black Performance Technologies},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3658854},
doi = {10.1145/3658852.3658854},
abstract = {This paper considers dance, ecological systems, and technology through the frame of critical race theories and histories. The authors describe an iterative, long form performance project to theorize its making and collaborative processes within a cultural context; the social construct of the “digital divide” alongside notions of Black Futurity and Afrofuturist praxis. The paper examines aspects of artwork to reveal synergies and ground embodied research through analysis of a project built between 2004-2016. Additionally, the essay tracks the activities of a multi-disciplinary collaborative team and the artists’ process through creative and empirical discoveries to better understand collaboration across diverse vectors and the paired roles of ecology and technology as both signifier and function. echo::system is the engendering of alternative landscapes deeply connected to culture and the environment in a way that postulates multiple affective realities of loss, heightened agency and presence, and possibility for the unknown. The project's constructed environments move audiences near to the environment through the distances we increasingly impose and inherit. Such inherited distance can be understood as industrial and colonial, enforced and false, driven by techno-ideological characteristics of the modern era. The projects’ presentation of dance and technology in a race-non-neutral performance reworks our sense and, importantly, sensorial experience of the environment and ourselves in its retooling of the technological descendants of the very mechanisms of our destruction. This paper combines a formal analysis of artistic practices, critical theory of race and technology, and inherent pressing conversations around algorithmic justice. It proposes challenging cultural logics through dance performance and art making as a provocation to others to extend and explore these concepts.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {1},
numpages = {9},
keywords = {Dance, Ecology, Performance, Race, Technology},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659065,
author = {Cotton, Kelsey and De Vries, Katja and Tatar, K\i{}van\c{c}},
title = {Singing for the Missing: Bringing the Body Back to AI Voice and Speech Technologies},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659065},
doi = {10.1145/3658852.3659065},
abstract = {Technological advancements in deep learning for speech and voice have contributed to a recent expansion in applications for voice cloning, synthesis and generation. Invisibilised stakeholders in this expansion are numerous absent bodies, whose voices and voice data have been integral to the development and refinement of these speech technologies. This position paper probes current working practices for voice and speech in machine learning and AI, in which the bodies of voices are “invisibilised". We examine the facts and concerns about the voice-Body in applications of AI-voice technology. We do this through probing the wider connections between voice data and Schaefferian listening; speculating on the consequences of missing Bodies in AI-Voice; and by examining how vocalists and artists working with synthetic Bodies and AI-voices are ‘bringing the Body back’ in their own practices. We contribute with a series of considerations for how practitioners and researchers may help to ‘bring the Body back’ into AI-voice technologies.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {2},
numpages = {12},
keywords = {AI, STS, artificial intelligence, body, musical AI, voice},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659066,
author = {Demers, Louis-Philippe and Vorn, Bill},
title = {Participative Robotics and The Spectacle of Motion Capture},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659066},
doi = {10.1145/3658852.3659066},
abstract = {Experiments with animating human bodies on stage are reported here. In our performances, animations are carried out by augmenting human motions, coercing actions, experiencing disruptions of realism, shifting kinesthetics, or altering anatomies. These activities are performed on the participants' bodies using mechatronic devices such as exoskeletons, supernumerary limbs, or symbiotic apparatuses. We adapt motion capture (mocap) techniques and data gathered from bodies, crowds, and devices to control, augment, and alter human movements as if we were animating 3D virtual characters, yet with the inevitable constraints of the physical world and human body integrity. In a constant shift of locus of perception, through 'ghost' control and gestural doubles, human volition is destabilized and turns into an uncanny paradox of pleasure and loss of self-control.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {3},
numpages = {6},
keywords = {Dance, Exoskeletons, Instruction Art, Live Performance, Motion Capture, Participative Art, Robotic Art, Supernumerary Limbs, Theatre, Wearables},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659079,
author = {den Hertog, Joyce and Saunders, Rob},
title = {SWDance: Transfer Learning a Text-To-Motion Model to Generate Choreography Conditioned on Spoken Word},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659079},
doi = {10.1145/3658852.3659079},
abstract = {This study introduces novel text-conditioned dance motion dataset SWDance, along with a transfer-learned diffusion model, MDMSWD, for generating dance sequences conditioned on spoken word text. To address the scarcity of dance datasets, particularly text-to-dance datasets, we propose a YouTube-sourced pipeline to collect text-to-motion data quickly and easily. Furthermore, this study is the first to generate dance motions based on non-descriptive text. Despite a neutral user preference, MDMSWD exhibited no significant disadvantage compared to ground truth. Participants expressed a strong interest in using an improved version of the model in their dance practice. The results of the study suggest exciting possibilities at the intersection of AI, dance and spoken word.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {4},
numpages = {4},
keywords = {Diffusion Models, Generative Dance, Text to Motion},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659067,
author = {Dertien, Edwin and Van Delden, Robby and Reidsma, Dennis},
title = {Improvisation Theatre as HRI simulation tool},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659067},
doi = {10.1145/3658852.3659067},
abstract = {This paper discusses how improvisation theatre can be used as a tool for design and (design) education in the field of Human-Robot Interaction. This tool has been explored in settings focused on design education for students as well as care professionals. In this paper, a format and set of guiding principles that were found to work constructively will be presented, as well as a discussion on outcomes, results, and insights from a number of sessions.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {5},
numpages = {8},
keywords = {Design education, Human Robot Interaction, Improv theatre},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659076,
author = {Endo, Koki and Tsuchida, Shuhei and Fukusato, Tsukasa and Igarashi, Takeo},
title = {Automatic Dance Video Segmentation for Understanding Choreography},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659076},
doi = {10.1145/3658852.3659076},
abstract = {Segmenting dance video into short movements is a popular way to easily understand dance choreography. However, it is currently done manually and requires a significant amount of effort by experts. That is, even if many dance videos are available on social media (e.g., TikTok and YouTube), it remains difficult for people, especially novices, to casually watch short video segments to practice dance choreography. In this paper, we propose a method to automatically segment a dance video into each movement. Given a dance video as input, we first extract visual and audio features: the former is computed from the keypoints of the dancer in the video, and the latter is computed from the Mel spectrogram of the music in the video. Next, these features are passed to a Temporal Convolutional Network (TCN), and segmentation points are estimated by picking peaks of the network output. To build our training dataset, we annotate segmentation points to dance videos in the AIST Dance Video Database, which is a shared database containing original street dance videos with copyright-cleared dance music. The evaluation study shows that the proposed method (i.e., combining the visual and audio features) can estimate segmentation points with high accuracy. In addition, we developed an application to help dancers practice choreography using the proposed method.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {6},
numpages = {9},
keywords = {dance practice, temporal convolutional networks, video segmentation},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3663339,
author = {Kalampratsidou, Vilelmini and Koutiva, Georgia and El Raheb, Katerina and Stergiou, Marina and Diamantides, Pandelis and Katifori, Akrivi and Ioannidis, Yannis and Issari, Philia and Georgaca, Eugenia and Karydi, Evangelia and Skali, Dora and Apostolopoulou, Antigoni and Papadopoulos, Nikolaos and Gkiokas, Panos and Vassilakou, Virginia and Pappas, Yannis},
title = {Here is an episode! Synchronizing and organizing citizens' biosignals for global artistic inspiration},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3663339},
doi = {10.1145/3658852.3663339},
abstract = {This work presents the design and development of a methodology investigating the ways that data deriving from human-embodied social experience can be synchronized, curated, and organized to be provided to artists for inspiration and integration in their creations. Given the new media artists’ increased interest in utilizing biosignals in their art-making, we experiment with data collected through wearable technologies during sociodrama sessions -public discussions between residents of selected communities facing social issues- from the participants’ embodied experience. While we consider biosignals to provide valuable insight into the unspoken communication that took place, they are framed by descriptions of the locals’ perspectives, interactions, and behaviors as observed by the sessions’ facilitators. The present case study revolved around the city of Eleusis, Greece. Utilizing data previously recorded in [7] during sociodrama sessions dedicated to its environmental, employment, and migration crises, we present our proposed methodology, which is designed to a) execute biosignal analysis to discover more about the embodied aspects of social interaction while simultaneously employing the outcomes of a psychology team’s qualitative sociodrama analysis (study [7]), b) provide this material through an online platform in a way that accommodates artistic needs when using biosignals, as detected in work [5], c) onboarding a community of well established global artists to create audiovisual works inspired by and using the material, and d) evaluating the utilizability of the platform and its material. Focusing on the heart rate, body temperature, and skin conductance of the participants, we start detailing the steps of data collection, extraction, and curation. These processes resulted in the detection of moments of intense biometric activity in the data, which we defined as episodes, and which served as the main structural unit of the data representation. Then, we describe the design of TransitionTo8 platform which organizes, presents, frames, and augments the collected data through descriptions of the social interactions and discussions that took place during the sociodrama sessions -the episodes’ transcripts, additional summaries, and commentary-, visualizations and sonifications. Next, we present the outcome of this methodology; artistic works created using the platform material and presented to the local community that inspired them in the form of a multimedia festival. Finally, we discuss whether our goal of distributing the curated data in a way that is meaningful and inspiring, enabling the connection between the sociodrama participant, the narratives that emerged during the sessions, and their embodied aspects, with artists from all over the world, was achieved.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {7},
numpages = {8},
keywords = {Social issues, biosignals, body temperature, heart rate, music composition, skin conductance, sociodrama},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659068,
author = {Kiliboz, Pelin and Erkut, Cumhur},
title = {Multimodal Looper: A Live-Looping System for Gestural and Audio-visual Improvisation},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659068},
doi = {10.1145/3658852.3659068},
abstract = {We present the Multimodal Looper, an embodied digital interface that connects body movements to audiovisual forms for musical improvisation. It extends the performative practice of the musical looper, which enables musicians to record and play back multiple layers of sound in real-time. With the multimodal looper, we explore music cognition from an embodied perspective, considering cross-modal correspondences in order to achieve an intuitive method suitable for collaboration. Our goal is to create a live-looping system with multimodal objects that are gesturally activated and visually represented. Our first prototype focused on the essential modules of a live-looping system: for gesture recognition, we used a depth camera and a decision tree; for visuals that correspond to distinct categories of sounds, e.g., sustained, iterative, and impulsive, we employed procedural generation techniques such as animated noise, feedback loops, instancing, and various mathematical operations. The system’s effectiveness in establishing cross-modal correspondences for a multisensory experience was evaluated through user testing.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {8},
numpages = {8},
keywords = {Embodied Interaction, Improvisation, Multimodal Systems, Visual Music},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3658862,
author = {Meshi, Avital},
title = {The New Vitruvian: Becoming-with an AI Recognition Algorithm},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3658862},
doi = {10.1145/3658852.3658862},
abstract = {The New Vitruvian is a performative interplay, a duet between a human artist and an AI recognition algorithm. The artist dances in front of the AI system and its watchful gaze. As the system processes and classifies her physical form, an intimate relationship is formed between the two, an entanglement within which new subjectivities emerge, identities are deconstructed and reconstructed repeatedly, and a new potential for freedom is revealed. This essay delves into the nuances of the performative practice, examining the intricate layers of The New Vitruvian, inviting readers to ponder the notion of identity transformation through the evolving relationship between humans and technologies.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {9},
numpages = {4},
keywords = {Artificial Intelligence, Creative AI, Dance, Performance Art, Posthumanism, Recognition Algorithms},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659070,
author = {Voillot, Marion and Matuszewski, Benjamin and Chevrier, Jo\"{e}l and Bevilacqua, Fr\'{e}d\'{e}ric},
title = {CoMo.education: Collective and embodied storytelling in kindergarten using a gesture-sound interactive system},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659070},
doi = {10.1145/3658852.3659070},
abstract = {Storytelling is an essential practice in preschool. Recent advances in digital tools for multimodal and collective interactions represent opportunities for new approaches in storytelling. Motivated by an embodied approach of storytelling, we developed CoMo•education, an interactive system where whole-body movements and sounds are in play. Moreover, this application enables storytelling as a collective and multimodal experience. Using gesture recognition with screenless smartphones, the teacher and pupils can use movements to play various soundscapes, which enliven the story. The aim of this article is to present the overall project, which has spanned more than 3 years of iterative developments. Importantly, it was developed through an iterative co-design process including designers, teachers, early-development experts and engineers. A user study was carried out in two kindergarten classes which allowed us to gather feedback from teachers and children, and formalize a series of guidelines for future works. A workshop was also dedicated to educators to evaluate how educators can appropriate the application to create interactive stories by themselves.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {10},
numpages = {11},
keywords = {child-computer interaction, early childhood education, embodied interaction, gesture-sound interactive system, storytelling},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659071,
author = {Martin, Patrick J and Sicchio, Kate and Chang, Gabriella and Dietzel, Charles},
title = {A Prototype Platform for Live Human-Robot Choreography},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659071},
doi = {10.1145/3658852.3659071},
abstract = {As robot sensors, mechanisms, and artificial intelligence algorithms increase in capability and availability, the performing arts has incorporated robots into different creative works. These performances often focus on humans and robots as separately controlled entities that interact at specific points in time. In this paper, we propose, implement, and evaluate a human-robot teaming platform that enables the live choreography of human-robot teams that work together within an improvisational performance context. This platform unifies robots and humans using a novel integration of programming abstractions, wireless haptics, and autonomous behaviors that provide a foundation to construct and manage non-dyadic human-robot teams. Furthermore, we demonstrate this platform in a live human-robot choreographic experience with close audience interaction. We use this performance to gather qualitative data about audience sentiment and performer experience that informs how these human-robot teams might be perceived when operating in close proximity.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {11},
numpages = {8},
keywords = {Autonomous Robots, Human-Robot Collaboration, Live Coding, Performing Arts},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659072,
author = {Nabi, Sarah and Esling, Philippe and Peeters, Geoffroy and Bevilacqua, Fr\'{e}d\'{e}ric},
title = {Embodied exploration of deep latent spaces in interactive dance-music performance},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659072},
doi = {10.1145/3658852.3659072},
abstract = {In recent years, significant advances have been made in deep learning models for audio generation, offering promising tools for musical creation. In this work, we investigate the use of deep audio generative models in interactive dance/music performance. We adopted a performance-led research design approach, establishing an art-research collaboration between a researcher/musician and a dancer. First, we describe our motion-sound interactive system integrating deep audio generative model and propose three methods for embodied exploration of deep latent spaces. Then, we detail the creative process for building the performance centered on the co-design of the system. Finally, we report feedback from the dancer’s interviews and discuss the results and perspectives. The code implementation is publicly available on our github1.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {12},
numpages = {9},
keywords = {HCI, dance-music-AI performance, deep learning, embodied exploration, generative models, latent space, motion-sound interaction},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3658864,
author = {Pajala-Assefa, Hanna},
title = {Choreographing in VR: Introducing ‘Substitute Performers’ as Informants in the Choreographic Process},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3658864},
doi = {10.1145/3658852.3658864},
abstract = {This paper is situated at the intersection of digital choreography and human-centered interaction within the scope of virtual reality. Both disciplines are discussed under the frame of contemporary artistic practice, an endeavor to make an artistic work while appropriating skills in crafting and composition. The paper focuses on the creation practice in and for virtual reality and the making of an artwork rooted in movement and with a choreographic idea of embedding an improvisatory score into the interaction design. Therefore, the thinking of choreography expands into the design of the interactions in the virtual realm and falls into the subfield of digital choreography.To reflect on the agent-entangled practice of digital choreography within VR and to trace the roles of various performative, epistemic agents active within the creative process, the author introduces a notion of substitute performers. The methods of phenomenology of lived mediated experience and digital choreography are synthesized to optimize the artistic endeavor to design for diverse bodies. These are then combined with various ethnographic methods to provide insights into moving and dancing bodies in a mediated performative realm. These intertwined methods were an elemental part of the creative process of the VR artwork Skeleton Conductor XR Art which was designed to induce pleasure through improvisational engagement and cultivate kinaesthetic awareness.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {13},
numpages = {8},
keywords = {Digital Choreography, Embodied design practice, Expanded Ethnography, Improvisatory systems},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3661317,
author = {Pataranutaporn, Pat and Mano, Phoomparin and Bhongse-Tong, Piyaporn and Chongchadklang, Tas and Archiwaranguprok, Chayapatr and Hantrakul, Lamtharn and Eaimsa-ard, Jirach and Maes, Pattie and Klunchun, Pichet},
title = {Human-AI Co-Dancing: Evolving Cultural Heritage through Collaborative Choreography with Generative Virtual Characters},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3661317},
doi = {10.1145/3658852.3661317},
abstract = {This research introduces an approach for translating traditional dance knowledge into interactive computational models extending beyond static dance performance recordings. Specifically, this paper presents the concept of "Human-AI co-dancing," which involves integrating human dancers with virtual dance partners powered by models derived from dance principles. To demonstrate this concept, the research focuses on the choreographic principles deconstructed from the knowledge of traditional Thai dance. The principles are analyzed and translated into computational procedures that dynamically manipulate the movements of a virtual character by altering animation keyframes and the motions of individual joints in real-time. We developed an interactive system that enables dancers to improvise alongside the virtual agent. The system incorporates voice control functionality, allowing the dancer, choreographer, and even the audience to participate in altering the choreography of the virtual agents by adjusting parameters that represent traditional Thai dance elements. Human-AI rehearsals yielded intriguing artistic results, with hybrid movement aesthetics emerging from the synergy and friction between humans and machines. The resulting dance production, "Cyber Subin," demonstrates the potential of combining intangible cultural heritage, intelligent technology, and posthuman choreography to expand artistic expression and preserve traditional wisdom in a contemporary context.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {14},
numpages = {10},
keywords = {AI-generated Character, Computer-generated choreography, Cultural Computing, Human-AI Interaction, Virtual Agent},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659080,
author = {Sharma, Paritosh and Filhol, Michael},
title = {Sign Language Synthesis using Pose Priors},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659080},
doi = {10.1145/3658852.3659080},
abstract = {The challenge of simulating realistic Sign Language using avatars lies in achieving accurate human-like postures for effective communication. Unlike artistic or motion capture techniques, linguist-driven procedural generation methods are widely employed, relying on skeletal representations to synthesize a broad range of signs. However, determining appropriate joint limits for these avatars is intricate due to inter-joint and intra-joint dependencies, as well as variations in biomechanical properties. In this context, our work addresses this problem by introducing a pose corrector, enhancing an established Sign Language synthesis technique. Focused on rectifying extreme joint rotations, our approach incorporates a pre-trained poser based on existing work, integrated with a 21-joint character model. The correction process involves applying linguist-defined constraints using AZee language and subsequent pose corrections, showcasing promising advancements in obtaining more natural sign gestures.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {15},
numpages = {4},
keywords = {Avatar, Motion Synthesis, Procedural Animation, Sign Language},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659074,
author = {Strauss, Lucy and Yee-King, Matthew},
title = {Towards a Machine Somaesthete: Latent Modeling of EMG Signals in Viola Playing},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659074},
doi = {10.1145/3658852.3659074},
abstract = {Somaesthetic experiential qualities can provide a window into process of meaning-making, both human and machinic. We draw such qualities from viola performance into the design-in-progress of a novel interactive performance system. In doing so, we introduce the concept of a Machine Somaesthete that senses and makes sense of these qualities from a second-person perspective. Our system comprises electromyographic (EMG) muscle sensing and a Variational Autoencoder. With a novel dataset, we aim to encode latent representations of performance movement that are meaningful from a somaesthetic perspective. We present our model and our design process, then analyse latent trajectories to interrogate how our system can be considered a Machine Somaesthete, and the nature of its sensitivity to bodily experiences of viola playing. At the intersection of artificial intelligence, music performance and intra-action design, we take a sympoietic (together-making) view of knowledge creation. We and our practices are transformed as we design - and design with - machine learning systems.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {16},
numpages = {11},
keywords = {Interactive Music Systems, Latent Modelling, Machine Learning, Machine Somaesthete, Soma-design, Sympoiesis},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659075,
author = {Trolland, Sam and Smith, Melinda and Ilsar, Alon and McCormack, Jon},
title = {Visual instrument co-design embracing the unique movement capabilities of a dancer with physical disability},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659075},
doi = {10.1145/3658852.3659075},
abstract = {This paper explores the design of an expressive visual instrument that embraces the unique movement style of a dancer living with physical disability. Through a collaboration between the dancer and an interaction designer/visual artist, the creative qualities of wearable devices for motion tracking are investigated, with emphasis on integrating the dancer’s specific movement capabilities with their creative goals. The affordances of this technology for imagining new forms of creative expression play a critical role in the design process. These themes are drawn together through an experiential performance which augments an improvised dance with an ephemeral real-time visualisation of the performer’s movements. Through practice-based research, the design, development and presentation of this performance work is examined as a ‘testbed’ for new ideas, allowing for the exploration of HCI concepts within a creative context. This paper outlines the creative process behind the development of the work, the insights derived from the practice-based research enquiry, and the role of movement technology in encouraging new ways of moving through creative expression.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {17},
numpages = {9},
keywords = {Dance, Gestural Interaction, Visualisation},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659077,
author = {Vincs, Kim and Mccormick, John and Mardamootoo, Pajani},
title = {Volumetric Interaction: a new approach to expanding embodied experience in XR},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659077},
doi = {10.1145/3658852.3659077},
abstract = {This paper presents a prototype for a new form of interaction termed ‘volumetric interaction’ (VI), developed by the authors, along with a specialist development team of programmers, 3D artists and dancers, in the Embodied Movement Design Studio within the Centre for Transformative Media Technologies in Melbourne, Australia. Current XR interaction is based on tracking users’ motion as a series of coordinates in space, which are mapped to virtual space to enable interaction within a virtual environment. VI proposes a new logic based on contemporary dance movement concepts that understand body movement as a series of shifting volumes rather than as a series of joint actions in space. This paper describes the creation and workshop testing of the first VI prototypes and reflects on the potential of VI to provide a new capacity to tailor interaction systems to differently abled people for whom gesture-based systems may not be suitable.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {18},
numpages = {8},
keywords = {Dance Knowledge, Dance Technology, Human Mesh Reconstruction, Motion Capture, SMPL},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659078,
author = {Xu, Ruilin and Tran, Vu An and Nayar, Shree K. and Krishnan, Gurunandan},
title = {DanceCraft: A Music-Reactive Real-time Dance Improv System},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659078},
doi = {10.1145/3658852.3659078},
abstract = {Automatic generation of 3D dance motion, in response to live music, is a challenging task. Prior research has assumed that either the entire music track, or a significant chunk of music track, is available prior to dance generation. In this paper, we present a novel production-ready system that can generate highly realistic dances in reaction to live music. Since predicting future music, or dance choreographed to future music, is a hard problem, we trade-off perfect choreography for spontaneous dance-motion improvisation. Given a small slice of the most recently received audio, we first determine where the audio include music, and if so extract high-level descriptors of the music such as tempo and energy. Based on these descriptors, we generate the dance motion. The generated dance is a combination of previously captured dance sequences as well as randomly triggered generative transitions between different dance sequences. Due to these randomized transitions, two generated dances, even for the same music, tend to appear very different. Furthermore, our system offers a high level of interactivity and personalization, allowing users to import their personal 3D avatars and have them dance to any music played in the environment. User studies show that our system provides an engaging and immersive experience that is appreciated by users.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {19},
numpages = {10},
keywords = {3D dance generation, deep learning, motion in-betweening, music-reactive, real-time},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659082,
author = {Caravati, Matteo and Tatar, K\i{}van\c{c}},
title = {Interfacing ErgoJr with Creative Coding Platforms},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659082},
doi = {10.1145/3658852.3659082},
abstract = {This paper introduces a project enabling non-coders to control a Poppy Ergo Jr. robotic arm with Dynamixel servomotors. Originally using a Raspberry Pi and Pixl board, various constraints related to importation led to adopting a ROBOTIS OpenCM9.04 board. A client-server architecture was implemented for remote control, with creative coding platforms (p5.js, Processing, Pure Data, Python) as clients. The server, utilizing a two-layer architecture, manages communication and interfaces with the ROBOTIS OpenCM9.04 board. The OSC and WebSocket protocols were chosen for communication due to their flexibility and their ease of use. Clients were developed for each platform, leveraging compatibility layers.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {20},
numpages = {4},
keywords = {C++, Creative coding, OSC, OpenCM9.04, Processing, Pure Data, Python, Robot control, WebSocket, p5.js},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659083,
author = {Correia, Nuno N. and Primett, William},
title = {Best Practices for Technology-Mediated Audience Interaction in Dance Performances},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659083},
doi = {10.1145/3658852.3659083},
abstract = {Artists have been progressively blurring the boundaries between audience and performer, particularly in technology-mediated performing arts. However, there is scarce literature systematising these approaches in the field of dance. This leads to our research question: What are the challenges with audience interaction in technology-mediated dance performances and what can be done to overcome them? To answer this question, we ran a focus group with 10 artists in the field of contemporary dance, with relevant and diverse experience. The analysis of the focus group allowed us to propose best practices for the design of audience interaction in technology-mediated dance performances. We also discuss these recommendations in light of existing literature.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {21},
numpages = {6},
keywords = {audience experience, audience interaction, contemporary dance, dance and technology, embodied interaction, human-computer interaction, movement and computing},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659084,
author = {Davis, Tommy and Pocius, Kasey and Cusson, Vincent and Gentili-Morin, Maxwell and Pasquier, Philippe},
title = {Embodied eTube Gestures and Agency},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659084},
doi = {10.1145/3658852.3659084},
abstract = {Following three years of working on the eTube with a collaborative interdisciplinary team, we describe the embodied and spatialized performance gestures developed in tandem with a microphone setup for interacting with musical improvising agents. Eric Lewis’ discussion of the intentional stance and make-believe are outlined as a way to conceptualize our working process and engagement with musical agents in improvisation. Within this context, we consider the various agencies at play, and how the musical agents challenge notions of the social and embodiment in improvisation. We will then describe certain artistic approaches and results that are afforded by merging the philosophical with practice. A collaboration with other artists will illustrate new eTube performance gestures. Finally, we outline spatialization models designed for the musical agents and how these were developed in the context of the eTube performance practice.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {22},
numpages = {6},
keywords = {Embodied performance practice, eTube, gesture, improvisation, musical agents, spatialization},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3658877,
author = {Donnelly, Leah},
title = {The Motion Augmented Acoustic Skate (MAAS) System: A Device for Real-Time Figure Skating Sonification},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3658877},
doi = {10.1145/3658852.3658877},
abstract = {This paper presents a device that transforms a skate’s sound and motion into a real-time auditory tool for figure skaters. Inspired by the skating tradition of “figures,” the Motion Augmented Acoustic Skate system enhances a skate’s existing acoustic feedback for non-jumping implementations. Using a contact microphone, an Inertial Measurement Unit (IMU), a radio transmitter, and a Wi-Fi microcontroller, the system wirelessly transmits sound and motion to an off-ice computer for signal processing. The resulting sonification output, altered through physical modeling and motion mappings, transmits back to the skater in real-time. The design and implementation of the Motion Augmented Acoustic Skate (MAAS) prototype is discussed as a future artistic, athletic, and somatic tool.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {23},
numpages = {6},
keywords = {figure skating, motion acoustics, movement sonification, scratch input, somatic sonification},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659085,
author = {Gunerli, John Hakan Can and Deshpande, Manoj and Magerko, Brian},
title = {Video Segmentation Pipeline For Co-Creative AI Dance Application},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659085},
doi = {10.1145/3658852.3659085},
abstract = {This article introduces a method that combines human input and computation for analyzing human motion from video recordings, specifically for capturing dance movements. The central aim is to develop an innovative system for processing and analyzing videos. This system consists of four key stages: using pre-trained MediaPipe models for interactive image segmentation, organizing videos efficiently through batching, identifying and extracting keyframes, and pinpointing accurate timestamps of keyframes. This pipeline is a part of LuminAI, an interactive installation that features a virtual AI agent capable of improvising movements in collaboration with human participants. In particular, the proposed pipeline fits into the first software module of LuminAI, responsible for recognizing and segmenting continuous motion capture data into separate body actions. The proposed video segmentation pipeline is designed to fulfill the requirements for both qualitative and quantitative analyses in designing systems that classify human movements. This research advances our knowledge of human motion through video analysis and bridges the gap between technology and artistic expression.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {24},
numpages = {5},
keywords = {Dance Movement Analysis, Human-AI Collaboration, Machine Learning in Art, Video Segmentation},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659086,
author = {Kantan, Prithvi Ravi and Rojo, Ana and Olmo-Fajardo, Tania and Matas, Lydia Martin and Spaich, Erika G. and Dahl, Sofia and Moreno, Juan C.},
title = {Developing an Integrated VR + Musical Feedback System for Stationary Biking in Endurance Training},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659086},
doi = {10.1145/3658852.3659086},
abstract = {Stationary biking is a common endurance training activity in the motor rehabilitation of neurological patients. Regular and intense training is crucial to eventual patient outcomes, but the activity itself is traditionally a monotonous one. In recent years, the potential of interactive virtual reality (VR) and musical feedback to enhance motor performance and motivation has been established, but there is a lack of proven training paradigms that optimally combine them to benefit patients. In this work, we iteratively developed an integrated multimodal interactive system to provide individualized VR and musical feedback in real time with the goal of boosting motivation and modulating pedaling intensity over a training session. The concept was initially realized by coupling two independent systems for VR and musical feedback on cumulative and instantaneous pedaling intensity respectively. The coupled systems were tested in a pilot study with 11 participants, and the overall paradigm was found to be usable and motivating with room for technical and design-specific improvement. In a second iteration of development, the systems were robustly integrated and the feedback design was streamlined to primarily inform on a single variable - cumulative pedaling intensity. With this new system built for individualized feedback delivery, the efficacy and clinical potential of the concept and system will be systematically evaluated in future work. The eventual results will, in our estimation, contribute to a growing body of knowledge on how best to leverage the potential of multimodal interactive systems in motor rehabilitation.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {25},
numpages = {6},
keywords = {Cycling, Interaction, Multimodal, Music, Rehabilitation, VR},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659087,
author = {Peng, Siyuan and Ladenheim, Kate and Shrestha, Snehesh and Ferm\"{u}ller, Cornelia},
title = {Generation of Novel Fall Animation with Configurable Attributes},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659087},
doi = {10.1145/3658852.3659087},
abstract = {It takes less than half a second for a person to fall [8]. Capturing the essence of a fall from video or motion capture is difficult. More generally, generating realistic 3D human body motions from motion capture (MoCap) data is a significant challenge with potential applications in animation, gaming, and robotics. Current motion datasets contain single-labeled activities, which lack fine-grained control over the motion, particularly for actions as sparse, dynamic, and complex as falling. This work introduces a novel human falling dataset and a learned multi-branch, Attribute-Conditioned Variational Autoencoder model to generate novel falls. Our unique dataset introduces a new ontology of the motion into three phases: Impact, Glitch, and Fall. Each branch of the model learns each phase separately and the fusion layer learns to fuse the latent space together. Furthermore, we present encompassing data augmentation techniques and an inter-phase smoothness loss for natural plausible motion generation. We successfully generated high quality images, validating the efficacy of our model in producing high-fidelity, attribute-conditioned human movements.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {26},
numpages = {6},
keywords = {Dataset, Fall, Human Body, Motion Synthesis, VAE},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659090,
author = {Saviano, Giuseppe and Villani, Alberto and Prattichizzo, Domenico},
title = {From Cage to Stage: Mapping Human Dance Movements Onto Industrial Robotic Arm Motion},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659090},
doi = {10.1145/3658852.3659090},
abstract = {The intersection of dance and robotics within the field of motion research consists of a fascinating artistic expression and technological innovation. Both disciplines leverage the study of movement for distinct purposes: dance delves into the field of emotion transmission, captivating audiences through the expressiveness of the human body, while robotics plans and controls artificial bodies to achieve practical objectives. In this context, we propose a novel approach to generate aesthetic choreographies for a robotic arm. Our methodology departs from the pragmatic nature of industrial tasks commonly assigned to the manipulator and seamlessly integrates the nuanced movements recorded by human dancers. Aesthetic and practical movements are decomposed into their Principal Components (PCs) before being mapped from the human to the robot, filling the gap between the amount of degree of freedom kinematic structures of both actors. This novel approach allows for the harmonious fusion of the precision-driven motions of the robotic arm with the fluent, emotive gestures proper of the dance. The principal components of dancer and robot movements are used to identify the fundamental elements that underlie both the functional purposes of the robotic arm and the artistic expressions of the human. In essence, our study aims to pave the way for new investigations both for dance and robotics, unlocking new possibilities for the convergence of art and technology. By bringing together the elegance of dance and the precision of robotics, we envision a future where machines not only perform tasks efficiently but also engage and inspire through the beauty and expressiveness of their movements.&nbsp;1},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {27},
numpages = {6},
keywords = {Robot dancing, fluid motion, robotics},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659081,
author = {Siopa, Jos\'{e} and Antunes, Rui Filipe and De Lima, Cec\'{\i}lia and Carrilho, Jo\~{a}o and Cl\'{a}udio, Ana Paula and Carmo, Maria Beatriz},
title = {LMA driven Dynamic Audiovisuals in a Virtual Reality Live Dance Performance: Ghostdance},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659081},
doi = {10.1145/3658852.3659081},
abstract = {Ghostdance is an evolving generative art project in the field of dance and virtual reality (VR). It mixes visual, auditory and immersive experiences, making use of generative algorithms to create a dynamic audiovisual landscape with continuously changing images and sounds. The performance consists of three interconnected components: a) a duet featuring a human dancer and an avatar mirroring the movements of an absent person; b) the transformation of the physical movements of the human dancer into a visualization of a hybrid body, constantly redefined as a swarm of virtual entities; and c) the sonification of the dancer's movements, introducing an auditory dimension to the exploration of movement.Performers dance in duets with virtual bodies, with pre-choreographed movements, in a visual and auditory landscape that evolves in real time due to adaptive generative algorithms responding to the presence and movements of the performer, informed by pretrained machine learning algorithms able to categorize the quality of the dancer's movement in Laban terms.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {28},
numpages = {5},
keywords = {Dance, Evolutionary audiovisuals, Live Performance, Virtual Reality},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659088,
author = {Stergiou, Marina and Baltas, Dimitris and Vosinakis, Spyros},
title = {Dancing with Themselves: An extended reality experiment for capturing partner dances.},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659088},
doi = {10.1145/3658852.3659088},
abstract = {Motion capture proves to be a powerful tool in the creation of interactive dance-related applications since it supports the recording and capturing of movement data, giving rise to multiple applications, including visualizing movement, analyzing movement, reflecting on movement, and providing feedback. Amongst its various applications, motion capture has emerged as a valuable means of promoting and preserving diverse forms of folk and traditional dances, contributing significantly to the preservation of cultural heritage. Pair dances, where two individuals engage in coordinated movements often in close physical proximity, are commonly met in the dance landscape. Such dances demand a high degree of cooperation, precise timing, and effective communication between the participating dancers. The conventional method for motion capturing pair dances involves simultaneously capturing the movements of both performers. However, this approach may pose challenges, including the need for two motion capture suits or sets of markers, which could be impractical for some studios, artists, or researchers due to cost constraints. To address the issue of motion capturing pair dancing with a single suit, one solution is to sequentially record each dancer and then combine the animations digitally. However, no matter how synchronized or well-rehearsed the dancers are, this method may not produce entirely realistic results, especially during moments of body contact or complex moves like spins. In response to this specific motion capture challenge, our work presents an Extended Reality (XR) experiment exploring alternative methods for capturing pair dances and examining how Extended Reality (XR) can enhance the practice, learning, or performance of pair dancing. The experimental concept involves two dancers performing a choreography, with the movements of one dancer (the leader) being motion captured. These captured movements are then assigned to a digital character, visualized in realistic size through a Virtual or Augmented reality headset. Subsequently, the second dancer (the follower) wears the motion capture suit and the headset, dancing alongside the digital character visible through the headset, while their movements are recorded. Afterwards, the two produced animations are combined, visualized and analyzed in a 3D environment. This approach aims to achieve greater synchronization between the two animations, resulting in more natural movements. For the experiment, we employed three visualization modalities for the digital partner: HoloLens (Augmented Reality), Oculus Quest2 (Virtual Reality), and Oculus Quest 2 Pass-through mode (Augmented Reality). We applied this methodology to two dance case studies: the Greek dance "Ballos" and the well-known Waltz dance. Through interviews, questionnaires with professional dancers, and the observation and analysis of animations in a 3D environment, we explored the strengths, constraints, and challenges of this proposed methodology, contributing valuable insights to the research areas of motion capture and digital dance partnering.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {29},
numpages = {6},
keywords = {Motion capture, augmented reality, pair dancing, virtual reality},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659089,
author = {Tokida, Satomi and Ishiguro, Yoshio},
title = {Dance with Rhythmic Frames: Improving Dancing Skills by Frame-by-Frame Presentation},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659089},
doi = {10.1145/3658852.3659089},
abstract = {Fitness video observation is a common approach in sports practice. However, in videos, important frames and others are presented at a constant speed, there is a substantial cognitive load in accurately capturing key movements and timing, especially in the quick-paced videos of complex activities such as dance. We hypothesize that extracting keyframes from dance videos and replaying them in sync with rhythm (frame-by-frame presentation) can reduce this cognitive load and improve the dance technique execution. Our first study using a 2D display suggests that frame-by-frame presentation is not only as preferred as conventional videos, but also enables more accurate learning of movements. Based on that, we developed DRF, a VR application that combines frame-by-frame presentation with motion trajectory visualization. User study results indicate that with DRF, users could significantly improve both choreographic dance technique and rhythm accuracy compared to video-based VR systems. Qualitative user evaluations from beginners, experienced dancers, and professionals expressed the benefits and potential use of the frame-by-frame presentation method.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {30},
numpages = {6},
keywords = {Education/Learning, Games/Play, Virtual/Augmented Reality},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659092,
author = {Atkins, Nick and Johnston, Andrew and Kocaballi, Baki},
title = {Message Bank},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659092},
doi = {10.1145/3658852.3659092},
abstract = {Message Bank&nbsp;is a performance made for audiences observing and navigating a public square. The performance tells the story of Charlie who, tasked with the mission of solving a small-scale crime, encounters an unidentified agent with the alias of ‘Dancing Bear.’ The audience takes on the role of Charlie and is asked to choose between the promise of safety with a state-based surveillance agency or the potential for disruption with a radical libertarian collective.&nbsp;Message Bank&nbsp;premiered in Parramatta Square as part of Sydney Festival 2023. The Sydney Festival season is referred to here as the first iteration of the performance and can be framed in the context of digital theatre [1], digital performance [2], and locative media [3]. Early examples of creative practice in this area include Teri Rueb's&nbsp;Trace, (1999) [4], Blast Theory's,&nbsp;Riderspoke, (2007) [5] and RATS Theatre's&nbsp;Maryam&nbsp;(2013) by Rebecca Forsberg [6]. More recently and locally to Australia; Leah Barclay's&nbsp;WIRA&nbsp;(2015) [7] offered audiences a geolocated audio walk along the Noosa River, Claudia Chidiac's&nbsp;The Village by the Kids&nbsp;(2022) [7] provided a neighborhood storytelling tour co-created with young people living in Bondi and Malthouse Theatre's&nbsp;Hour of the Wolf (2023) [9] used FM transmission to facilitate an immersive multi-room murder mystery within the interior of a theatre. The form of these works’ ranges from installation to digital performance and digital theatre but each are similar in their attempts to make meaning between place, story and an audience in motion.&nbsp;Message Bank&nbsp;builds on the practice-based knowledge generated through these works and contributes further insights to how they might enliven public spaces. The use of the term enlivening here is inspired by Auslander's work on liveness [6] as it relates to spontaneity, community, feedback, and presence. The&nbsp;Message Bank&nbsp;team consists of a director / researcher, two additional writers, a digital artist, designer, composer, outside eye, production observer, creative producer, and actors. The team collaborated intermittently over a period of five months in which the project was conceived, workshopped, drafted through script and application development, recorded as audio and video extracts, and tested through prototypes. Discoveries from the first iteration in the areas of motion, interaction, co-present audiences, and the role of time have fed into further development of the performance leading to the second iteration of&nbsp;Message Bank&nbsp;presented at MOCO. This second iteration maintains the vision while further exploring the performances capacity for linking audiences together through handheld devices and a mix of real-time spatiotemporal data to enliven public squares. Working between digital theatre, digital performance, and locative media the creative team is engaged in an ongoing process of iteration that mixes dramaturgy and technical experimentation. This manifests through the ongoing development of&nbsp;Message Bank's&nbsp;narrative and the design of the accompanying software which facilitates the experience. The second iteration presented at MOCO has a specific focus on how the performance binds co-present audiences both intended and unintended as well as how it connects motion data to the revelation of story beats. &nbsp;},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {31},
numpages = {2},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659093,
author = {Corness, Greg and Carlson, Kristin and Cherepanova, Sargylana},
title = {Catching the Inverse Ghosts: A Virtual Reality Installation},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659093},
doi = {10.1145/3658852.3659093},
abstract = {In the physical world, we as humans are used to consciously utilizing our space to help navigate our balance, directions, and intention. This virtual reality installation enables the the mover/ player to explore their visible relationships to space by using a Mind’s Eye perspective to playfully interject themselves in a virtual space.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {32},
numpages = {3},
keywords = {Game Engines, Interaction Design, Movement, Spatial Relationships},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3661318,
author = {Hosale, Mark-David and Macy, Alan J. and Jochum, Elizabeth and Overholt, Dan},
title = {BEAM Workshop: Biophysical Expression, Affect \&amp; Movement},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3661318},
doi = {10.1145/3658852.3661318},
abstract = {The workshop explores how interactive technologies based on biophysical sensing and analysis can help create deeper connections with the surrounding world and one another, by combining the latest affect-indicating biosensors with somatic practices and creative expression. The workshop incorporates cutting edge biophysical sensing measurement technologies to explore new modes of artistic expression that directly gauge human affect as an integral component in the creation of an art work or performance. The goal is to explore how real-time tracking of a participant’s affective state can be used as an interactive modality in performance and computational art. Human affect data is gathered from a custom biophysical sensing acquisition and analysis system called, The Source. Data is accessed in real-time using custom software tools built on popular platforms such as Arduino, Max/MSP, SuperCollider, Ableton Live, TouchDesigner, and Processing. Physiological measures, including Electrocardiography (ECG, heart rate), Electrodermal Activity (EDA), Electromyography (EMG, muscle), Electroencephalography (EEG, brain waves), Electrooculography (EOG, eye movement), and Respiratory effort (RSP, breathing) can be incorporated. During the workshop, we will explore the potential for creative uses, as well as more applied contexts to support physical therapy, group training and experiences.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {33},
numpages = {3},
keywords = {Affective States, Augmented Performance, Biosensing, Interactive Systems, Wearable Computing},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659094,
author = {Ladenheim, Kate and Bendell, Mollye and Kelly, Timothy},
title = {COMMIT!: Interactive Performance Reveals Social and Technological Mechanics of Control},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659094},
doi = {10.1145/3658852.3659094},
abstract = {This paper describes COMMIT!, an ongoing performance experiment that uses motion capture, bio-mechanical sensing, and interactive web platforms to consider the elusive, changeable nature of “commitment” and its relation to technological surveillance and game-like interaction mechanics.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {34},
numpages = {3},
keywords = {avatar, dance, expressive notation, interactive performance, motion capture},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3659095,
author = {Mccormick, John and Pyaraka, Jagannatha Charjee},
title = {Real Robot Dance Challenge: Exploring live and online dance challenge videos for robot movement},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3659095},
doi = {10.1145/3658852.3659095},
abstract = {This paper describes a proposed robotic artwork for the MOCO 2024 conference. Real Robot Dance Challenge is a human-robot art installation that investigates both live human-robot interactions as well as human movement embedded in online videos as sources for the robot's movement. Dance is extremely challenging for a robot in terms of movement goals. Compared to typical industrial robot approaches, actions may have a comparative excess expenditure of energy, and style and expressivity becoming more important than accuracy and control. This work draws on the popular trend of dance challenges as seen on platforms such as Tik Tok [1]. The short format dances, often 10 – 30 seconds long, are choreographed with the intention of showcasing skills as well as challenging others to reinterpret the dance, often done with individual stylistic changes. RRDC challenges the robot to reinterpret the movement of human movers interacting live as well as dances contained in online videos. The artwork leverages the unity game engine as a mediating platform for the robot to access human movement and repurpose it for its own body. This allows the robot's movement to be procured from multiple sources. The unity environment can also be used to connect robots of different morphologies to the movement sources enabling flexibility in the engagement between human and robot. The robot moves beyond the control of programmed actions, able to respond with interpretations of the movement offered to it.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {35},
numpages = {4},
keywords = {AI, Human Robot Interaction, Robot Dance, Robot Dance Challenge, Robot Installation, Unity},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3658852.3658889,
author = {Young, Megan},
title = {Immersive Experiences in Socially Engaged VR: Exploring the uniquely human vernacular of “With What?”},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ins2i.bib.cnrs.fr/10.1145/3658852.3658889},
doi = {10.1145/3658852.3658889},
abstract = {With What? represents an unfolding collective discourse around technology as ancestry. It situates a socially engaged virtual reality (VR) experience within a lightweight visual art installation that introduces a writing prompt and crafting station. The project asks what we carry across borders and through time. Participants answer through written responses and by exploring ritualized paths across physical and digital spaces. The custom VR landscape, developed in Unity, introduces an experimental archive of familial objects, femme figures, and audio recordings gathered through creative workshops and residencies. Participants traverse the VR world by following predetermined pathways and discovering artifacts as they travel. This novel interaction design invites kinesthetically grounded inquiry, with viewers collecting visual, tactile, and contextual insights via improvisational methods. The project considers how socially engaged VR may highlight artifacts of experience, reflecting our uniquely human vernacular.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {36},
numpages = {4},
keywords = {experimental archiving, human vernacular, social practice},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

